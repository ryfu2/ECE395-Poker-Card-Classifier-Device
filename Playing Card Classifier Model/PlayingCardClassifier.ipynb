{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README\n",
    "\n",
    "I followed the attached link to learn how pytorch neural nets work and and how to create a playing card classifier.\n",
    "Note: Python 3.12 didn't work, try downgrading if so.\n",
    "\n",
    "https://www.youtube.com/watch?v=tHL5STNJKag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision timm matplotlib pandas numpy tqdm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # provides neural network functions like convolution layers\n",
    "import torch.optim as optim # provides optimizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision # helps working with images easier\n",
    "import torchvision.transforms as transforms \n",
    "from torchvision.datasets import ImageFolder\n",
    "import timm # pytorch image model library, pretrained weights optimized for image classification\n",
    "\n",
    "import matplotlib.pyplot as plt # data visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm # for progress bar\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "\n",
    "# determine if want to train new model or use exists\n",
    "train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Set up data set and date loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthethic images\n",
    "\n",
    "def log2img(class_name):\n",
    "    pixel_data = []\n",
    "    found_start = False\n",
    "    log_path = f\"log_data/{class_name}.log\"\n",
    "    with open(log_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        for line in file:\n",
    "            # skip straight to the data\n",
    "            if not found_start:\n",
    "                if \"index: 0\" in line:\n",
    "                    found_start = True\n",
    "                continue\n",
    "            else:\n",
    "                parts = line.split(',')\n",
    "                for part in parts:\n",
    "                    if 'data:' in part:\n",
    "                        # Extract the hex after 'data:'\n",
    "                        word = part.split(': ')[1].strip().zfill(8)\n",
    "                        # word = [R1, G1, B1, R0, G0, B0]\n",
    "                        # print(f\"word: {word}\")\n",
    "                        int_val = int(word, 16)  # Convert hexadecimal string to integer\n",
    "\n",
    "                        # Extract individual R, G, and B components\n",
    "                        R0 = ((int_val >> 27) & 0b11111) << 3\n",
    "                        G0 = ((int_val >> 21) & 0b111111) << 2\n",
    "                        B0 = ((int_val >> 16) & 0b11111) << 3\n",
    "                        R1 = ((int_val >> 11) & 0b11111) << 3\n",
    "                        G1 = ((int_val >> 5) & 0b111111) << 2\n",
    "                        B1 = (int_val & 0b11111) << 3\n",
    "\n",
    "                        p0 = [R0, G0, B0]\n",
    "                        p1 = [R1, G1, B1]\n",
    "                        \n",
    "                        pixel_data.append(p1)\n",
    "                        pixel_data.append(p0)\n",
    "\n",
    "        height = 148\n",
    "        width = 172\n",
    "        data = np.array(pixel_data)\n",
    "        target_shape = (height, width, 3)\n",
    "\n",
    "        image_data = np.zeros((height * width, 3), dtype=np.uint8)\n",
    "        image_data[:len(data)] = data\n",
    "        image_data = image_data.reshape(target_shape)\n",
    "        image = Image.fromarray(image_data)\n",
    "        return image\n",
    "\n",
    "def random_rotate_180(img):\n",
    "    return img.rotate(180) if random.random() < 0.5 else img\n",
    "\n",
    "# transform applied to all images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.02),\n",
    "    transforms.RandomPerspective(distortion_scale=0.25, p=0.5),\n",
    "    transforms.Lambda(random_rotate_180), # handle card orientation\n",
    "    transforms.RandomRotation(degrees=(-6,6)), # to handle camera rotation\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# creates all the datasets for a specific class_name\n",
    "def create_class_dataset(class_name):\n",
    "    image = log2img(class_name)\n",
    "    dataset_size = 30 # number of images per folder\n",
    "    dataset_folders = [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "    for folder in dataset_folders:\n",
    "        # make dir if doenst eist\n",
    "        image_dir = f\"dataset/{folder}/{class_name}\"\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        if folder == \"train\":\n",
    "            # save og image into training set\n",
    "            image.save(os.path.join(image_dir, f\"{class_name}-0.jpg\"))\n",
    "\n",
    "        # create and save syntehtic images\n",
    "        for i in range(1, dataset_size):\n",
    "            synthetic_image = transform(image)\n",
    "            synthetic_image_pil = transforms.ToPILImage()(synthetic_image)\n",
    "            synthetic_image_pil.save(os.path.join(image_dir, f\"{class_name}-{i}.jpg\"))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all data sets by iterating througho the log_data\n",
    "\n",
    "if train:\n",
    "    for log in os.listdir(\"log_data\"):\n",
    "        class_name = os.path.splitext(log)[0]\n",
    "        create_class_dataset(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayingCardDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data = ImageFolder(data_dir, transform=transform) # Creates classes using folder name\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    @property\n",
    "    def classes(self):\n",
    "        return self.data.classes\n",
    "\n",
    "# transform input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)), # resize input image to desired pixel dimensions\n",
    "    transforms.ToTensor(), # converts pixel RGB val from [0,255] -> [0,1]\n",
    "])\n",
    "\n",
    "train_dir = \"dataset/train\"\n",
    "valid_dir = \"dataset/valid\"\n",
    "test_dir = \"dataset/test\"\n",
    "\n",
    "train_dataset = PlayingCardDataset(train_dir, transform)\n",
    "valid_dataset = PlayingCardDataset(valid_dir, transform)\n",
    "test_dataset = PlayingCardDataset(test_dir, transform)\n",
    "\n",
    "# Data loading\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 2: Design Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardClassifer(nn.Module):\n",
    "    def __init__(self, num_classes=54):\n",
    "        super(CardClassifer, self).__init__()\n",
    "\n",
    "        # note: could design image classification architecture ourself by defining each layer\n",
    "        # however, using an optimized image classificatino model with pretrained weights using timm\n",
    "        self.base_model = timm.create_model('efficientnet_b0', pretrained=True)  \n",
    "\n",
    "        self.features = nn.Sequential(*list(self.base_model.children())[:-1])\n",
    "\n",
    "        enet_out_size = 1280 # default size of the efficientnet_b0, we will resize enout into our num of classe later\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(enet_out_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Connect these parts and return the output\n",
    "        x = self.features(x)\n",
    "        output = self.classifier(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Train + Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: want to validate the model on data it hasn't been trained on => split data into train and valid dataset.\n",
    "# Terms: Epoch = one run through entire training dataset, step = one batch of data\n",
    "\n",
    "# general idea: Load data in model in batches, then calculate loss and perform backpropagation to modify weights starting from last layer to minimize that loss\n",
    "\n",
    "# training parameters\n",
    "num_epochs = 6\n",
    "train_losses, val_losses = [], []\n",
    "model = CardClassifer()\n",
    "criterion = nn.CrossEntropyLoss() # loss function (what model optimizes to minimize loss)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader, desc='Training loop'):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images) # call forward on the images\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward() # backpropagation to update model weight\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc='Validation loop'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "        val_loss = running_loss / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss}, Validation loss: {val_loss}\")\n",
    "else: # load in already trained model\n",
    "    # model = timm.create_model('efficientnet_b0', pretrained=False)  # define architetcture of model\n",
    "    model_weights_path = \"model.pth\"\n",
    "    model.load_state_dict(torch.load(model_weights_path))\n",
    "    model.eval() # set to eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Loss\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss over epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)), # resize input image to desired pixel dimensions\n",
    "    transforms.ToTensor(), # converts pixel RGB val from [0,255] -> [0,1]\n",
    "])\n",
    "\n",
    "def preprocess_image(image_path, transform):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return image, transform(image).unsqueeze(0)\n",
    "\n",
    "# Predict using the model\n",
    "def predict(model, image_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        probabilities = probabilities.cpu().numpy().flatten()\n",
    "        max_idx = np.argmax(probabilities)\n",
    "        predicted_class = train_dataset.classes[max_idx]\n",
    "    return predicted_class, probabilities\n",
    "\n",
    "# Visualization\n",
    "def visualize_predictions(original_image, predicted_class, probabilities, class_names):\n",
    "    fig, axarr = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    \n",
    "    # Display image\n",
    "    axarr[0].imshow(original_image)\n",
    "    axarr[0].axis(\"off\")\n",
    "    axarr[0].set_title(f\"Prediction: {predicted_class}\")\n",
    "    \n",
    "    # Display predictions\n",
    "    axarr[1].barh(class_names, probabilities)\n",
    "    axarr[1].set_xlabel(\"Probability\")\n",
    "    axarr[1].set_title(\"Class Predictions\")\n",
    "    axarr[1].set_xlim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy using test data\n",
    "label_to_class = {v: k for k, v in ImageFolder(test_dir).class_to_idx.items()}\n",
    "\n",
    "sample_size = 10\n",
    "num_test_images = 0\n",
    "num_correct = 0\n",
    "random_idxes = random.sample(range(0, len(test_dataset)), sample_size)\n",
    "\n",
    "for i in range(sample_size):\n",
    "    if num_test_images == sample_size:\n",
    "        break\n",
    "    random_idx = random_idxes[i]\n",
    "    image, label = test_dataset[random_idx]\n",
    "    image_tensor = image.unsqueeze(0) # makes it a batch size of 1 when model expects a batch of inages as input\n",
    "    predicted_class, probabilities = predict(model, image_tensor)\n",
    "    num_test_images += 1\n",
    "    if predicted_class == label_to_class[label]:\n",
    "        num_correct += 1\n",
    "    else: # display if incorrect\n",
    "        original_image = transforms.ToPILImage()(image)\n",
    "        visualize_predictions(original_image, predicted_class, probabilities, class_names)\n",
    "\n",
    "    class_names = train_dataset.classes\n",
    "print(f\"Accuracy: {num_correct} / {sample_size} Correct ({100 * num_correct / sample_size}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "test_images = glob(\"dataset/test/*/*\")\n",
    "test_examples = np.random.choice(test_images, 1)\n",
    "\n",
    "for example in test_examples:\n",
    "    original_image, image_tensor = preprocess_image(example, transform)\n",
    "    predicted_class, probabilities = predict(model, image_tensor)\n",
    "\n",
    "    class_names = train_dataset.classes \n",
    "    visualize_predictions(original_image, predicted_class, probabilities, class_names)\n",
    "\n",
    "# manual test\n",
    "\n",
    "def preprocess_user_image(image_path, transform):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    # image = image.rotate(90) # must be sideways\n",
    "    return image, transform(image).unsqueeze(0)\n",
    "\n",
    "user_image_name = \"DA-2.jpg\"\n",
    "user_image_path = f\"dataset/usertest/{user_image_name}\"\n",
    "original_image, image_tensor = preprocess_image(user_image_path, transform)\n",
    "print(image_tensor.shape)\n",
    "predicted_class, probabilities = predict(model, image_tensor)\n",
    "\n",
    "# Assuming dataset.classes gives the class names\n",
    "class_names = train_dataset.classes \n",
    "visualize_predictions(original_image, predicted_class, probabilities, class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Save the state_dict of the model as a pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save .pth (trained weighted sums)\n",
    "if train:\n",
    "    model_path = \"model.pth\"\n",
    "    model.eval()\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: To use on STM32, convert to tflite model and quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nvidia-pyindex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install onnx_graphsurgeon onnx2tf sng4onnx tensorflow==2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import onnx2tf\n",
    "\n",
    "\n",
    "input_shape = (1, 3, 128, 128)\n",
    "\n",
    "# Step 1: Convert PyTorch model to ONNX\n",
    "dummy_input = torch.randn(input_shape)  # Example input tensor\n",
    "torch.onnx.export(model, dummy_input, \"model.onnx\", export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2:Convert ONNX -> TF\n",
    "!onnx2tf -i model.onnx -b 1 -osd -cotof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test tf model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((128, 128))  # Resize the image to match model input size\n",
    "    img = (np.array(img) / 255.0).astype(np.float32)  # Normalize pixel values to [0, 1], and is type float32\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension # tf tensor = (1, 128, 128, 3)\n",
    "    return img\n",
    "\n",
    "# Test the model with an input image\n",
    "def test_model_with_image(image_path, model):\n",
    "    # Preprocess the image\n",
    "    img = preprocess_image(image_path)\n",
    "    # Perform inference\n",
    "    prediction = model(img)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# Load tf model\n",
    "model = tf.saved_model.load(\"saved_model\")\n",
    "\n",
    "image_name = \"C8/C8-2.jpg\"\n",
    "image_path = f\"dataset/test/{image_name}\"\n",
    "output_data = test_model_with_image(image_path, model)\n",
    "print(\"Output:\", output_data)\n",
    "\n",
    "max_index = np.argmax(output_data)\n",
    "label_to_class = {v: k for k, v in ImageFolder(test_dir).class_to_idx.items()}\n",
    "\n",
    "print(f\"prediction: {label_to_class[max_index]} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 167, Total Ops 413, % non-converted = 40.44 %\n",
      " * 167 ARITH ops\n",
      "\n",
      "- arith.constant:  167 occurrences  (f32: 164, i32: 3)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 9)\n",
      "  (f32: 65)\n",
      "  (f32: 16)\n",
      "  (f32: 1)\n",
      "  (f32: 65)\n",
      "  (f32: 17)\n",
      "  (f32: 65)\n",
      "  (f32: 5)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "# convert tf -> tflite\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "def rep_dataset():\n",
    "    \"\"\"Generator function to produce representative dataset for post-training quantization.\"\"\"\n",
    "    subset_indices = list(range(100))  # Indices of the first 100 samples\n",
    "    subset = Subset(valid_dataset, subset_indices)\n",
    "    # Use a few samples from the training set.\n",
    "    for image_tensor, label in subset: # pytorch image tensor = (1,3, 128,128)\n",
    "        image_tensor = image_tensor.unsqueeze(0) # add batch dimension\n",
    "        image_tensor = image_tensor.permute(0, 2, 3, 1)  # Transpose dimensions to match tensor flow order of (1, 128, 128, 3)\n",
    "        yield [tf.dtypes.cast(image_tensor, tf.float32)]\n",
    "\n",
    "\n",
    "# Quantize the TF model = 8-bit linear quantization of an NN model \n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"saved_model\")\n",
    "converter.signature_key = \"serving_default\"\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] # st might've warned not to use this. try commneting out if dones twork\n",
    "converter.representative_dataset = rep_dataset\n",
    "\n",
    "# Ensure that if ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "converter.inference_input_type = tf.float32 # note: uint8 is far too inaccuarte\n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "# Convert TF -> TFLITE\n",
    "tflite_quantized_model = converter.convert()\n",
    "with open(\"model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [[-2.4195955 -1.5281656 -2.1649013 -7.5134807 -4.7118435 -6.7493978\n",
      "  -5.2212324 -7.0040917 -4.966538  -5.3485794 -6.3673563 -4.5844965\n",
      "  -6.622051  -7.8955216  0.6367356  4.966538  -6.1126623 -6.3673563\n",
      "  -4.966538  -5.093885  -4.3298025 -7.6408277 -5.985315  -3.3110254\n",
      "  -3.8204138 -5.857968  -4.839191  -2.6742897 -2.1649013 -7.386133\n",
      "  -8.659605  -5.985315  -4.7118435 -4.5844965 -6.1126623 -4.7118435\n",
      "  -6.1126623 -5.2212324 -6.7493978 -5.093885  -3.8204138 -2.1649013\n",
      "   7.131439  -6.622051  -2.928984  -5.6032734 -4.839191  -5.4759264\n",
      "  -6.7493978 -3.3110254 -2.8016367 -3.5657196 -3.9477608 -5.6032734]]\n",
      "S3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "# test tflite model\n",
    "\n",
    "def preprocess_image(image_path, input_details):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((input_details[0]['shape'][2], input_details[0]['shape'][1]))  # Resize image to model input shape (128, 128)\n",
    "    img = (np.array(img) / 255.0).astype(np.float32)  # Normalize pixel values to [0, 1], and is type float32 # note: uint8 is far too inaccuarte\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "def test_model_with_image(image_path, interpreter):\n",
    "    # Get input and output tensors\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    img = preprocess_image(image_path, input_details)\n",
    "    interpreter.set_tensor(input_details[0]['index'], img) # Set input tensor\n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "    # Get the output tensor\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output_data\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "image_name = \"H4/H4-10.jpg\"\n",
    "image_path = f\"dataset/test/{image_name}\"\n",
    "output_data = test_model_with_image(image_path, interpreter,)\n",
    "\n",
    "print(\"Output:\", output_data)\n",
    "max_index = np.argmax(output_data)\n",
    "label_to_class = {v: k for k, v in ImageFolder(test_dir).class_to_idx.items()}\n",
    "print(label_to_class[max_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
