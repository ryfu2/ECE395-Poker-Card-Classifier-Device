{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README\n",
    "\n",
    "I followed the attached link to learn how pytorch neural nets work and and how to create a playing card classifier.\n",
    "Note: Python 3.12 didn't work, try downgrading if so.\n",
    "\n",
    "https://www.youtube.com/watch?v=tHL5STNJKag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision timm matplotlib pandas numpy tqdm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # provides neural network functions like convolution layers\n",
    "import torch.optim as optim # provides optimizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision # helps working with images easier\n",
    "import torchvision.transforms as transforms \n",
    "from torchvision.datasets import ImageFolder\n",
    "import timm # pytorch image model library, pretrained weights optimized for image classification\n",
    "\n",
    "import matplotlib.pyplot as plt # data visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm # for progress bar\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "train = True # determine if want to train new model or use exists\n",
    "have_data = True # set to false if dont have any data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that MPS is available, Optimize performance on Apple Silicon Processors\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Selected device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Set up data set and date loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthethic images\n",
    "\n",
    "def log2img(class_name):\n",
    "    pixel_data = []\n",
    "    found_start = False\n",
    "    log_path = f\"log_data/{class_name}.log\"\n",
    "    with open(log_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        for line in file:\n",
    "            # skip straight to the data\n",
    "            if not found_start:\n",
    "                if \"index: 0\" in line:\n",
    "                    found_start = True\n",
    "                continue\n",
    "            else:\n",
    "                parts = line.split(',')\n",
    "                for part in parts:\n",
    "                    if 'data:' in part:\n",
    "                        # Extract the hex after 'data:'\n",
    "                        word = part.split(': ')[1].strip().zfill(8)\n",
    "                        # word = [R1, G1, B1, R0, G0, B0]\n",
    "                        # print(f\"word: {word}\")\n",
    "                        int_val = int(word, 16)  # Convert hexadecimal string to integer\n",
    "\n",
    "                        # Extract individual R, G, and B components\n",
    "                        R0 = ((int_val >> 27) & 0b11111) << 3\n",
    "                        G0 = ((int_val >> 21) & 0b111111) << 2\n",
    "                        B0 = ((int_val >> 16) & 0b11111) << 3\n",
    "                        R1 = ((int_val >> 11) & 0b11111) << 3\n",
    "                        G1 = ((int_val >> 5) & 0b111111) << 2\n",
    "                        B1 = (int_val & 0b11111) << 3\n",
    "\n",
    "                        p0 = [R0, G0, B0]\n",
    "                        p1 = [R1, G1, B1]\n",
    "                        \n",
    "                        pixel_data.append(p1)\n",
    "                        pixel_data.append(p0)\n",
    "\n",
    "        height = 148\n",
    "        width = 172\n",
    "        data = np.array(pixel_data)\n",
    "        target_shape = (height, width, 3)\n",
    "\n",
    "        image_data = np.zeros((height * width, 3), dtype=np.uint8)\n",
    "        image_data[:len(data)] = data\n",
    "        image_data = image_data.reshape(target_shape)\n",
    "        image = Image.fromarray(image_data)\n",
    "        return image\n",
    "\n",
    "def random_rotate_180(img):\n",
    "    return img.rotate(180) if random.random() < 0.5 else img\n",
    "\n",
    "# transform applied to all images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.02),\n",
    "    transforms.RandomPerspective(distortion_scale=0.25, p=0.5),\n",
    "    transforms.Lambda(random_rotate_180), # handle card orientation\n",
    "    transforms.RandomRotation(degrees=(-6,6)), # to handle camera rotation\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# creates all the datasets for a specific class_name\n",
    "def create_class_dataset(class_name):\n",
    "    image = log2img(class_name)\n",
    "    dataset_size = 30 # number of images per folder\n",
    "    dataset_folders = [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "    for folder in dataset_folders:\n",
    "        # make dir if doenst eist\n",
    "        image_dir = f\"dataset/{folder}/{class_name}\"\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        if folder == \"train\":\n",
    "            # save og image into training set\n",
    "            image.save(os.path.join(image_dir, f\"{class_name}-0.jpg\"))\n",
    "\n",
    "        # create and save syntehtic images\n",
    "        for i in range(1, dataset_size):\n",
    "            synthetic_image = transform(image)\n",
    "            synthetic_image_pil = transforms.ToPILImage()(synthetic_image)\n",
    "            synthetic_image_pil.save(os.path.join(image_dir, f\"{class_name}-{i}.jpg\"))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all data sets by iterating througho the log_data\n",
    "if have_data == False:\n",
    "    for log in os.listdir(\"log_data\"):\n",
    "        class_name = os.path.splitext(log)[0]\n",
    "        create_class_dataset(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayingCardDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data = ImageFolder(data_dir, transform=transform) # Creates classes using folder name\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    @property\n",
    "    def classes(self):\n",
    "        return self.data.classes\n",
    "\n",
    "# transform input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)), # resize input image to desired pixel dimensions\n",
    "    transforms.ToTensor(), # converts pixel RGB val from [0,255] -> [0,1]\n",
    "])\n",
    "\n",
    "train_dir = \"dataset/train\"\n",
    "valid_dir = \"dataset/valid\"\n",
    "test_dir = \"dataset/test\"\n",
    "\n",
    "train_dataset = PlayingCardDataset(train_dir, transform)\n",
    "valid_dataset = PlayingCardDataset(valid_dir, transform)\n",
    "test_dataset = PlayingCardDataset(test_dir, transform)\n",
    "\n",
    "# Data loading\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 2: Design Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardClassifierCNN(nn.Module):\n",
    "    def __init__(self, num_classes=54):\n",
    "        super(CardClassifierCNN, self).__init__()\n",
    "\n",
    "        # convolution layer output size = (input_image_size - kernel_size + 2*padding)/stride + 1\n",
    "        # image = 3 x 128 x 128\n",
    "        input_image_size = 128\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=2, padding=0) # 3 = input channels (RGB), 64 = output channel = depth of array\n",
    "\n",
    "        # image = 6 x (input_image_size - kernel_size + 1)/2 = 6 x 62 x 62\n",
    "        self.relu1 = nn.ReLU() # add non linearity using reLU activation function (doesnt change size)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride = 2) # pooling layer reduces image by factor of 2\n",
    "\n",
    "        # image = 6 x (input_image_size - kernel_size + 1)/2 = 6 x 31 x 31\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=2, stride=1, padding=0) # 6 = out_channel of prev conv layer\n",
    "\n",
    "        # image = 16 x 30 x 30\n",
    "        \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride = 2) # pooling layer reduces image by factor of 2\n",
    "\n",
    "        # image = 16 x 15 x 15\n",
    "\n",
    "        # flatten 3d convolution layer's tensor into 1d tensor\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # transition convolutino layer into fully connected layers\n",
    "        self.fc3 = nn.Linear(16*15*15, 120) # 16 = out_channel of final conv layer, 29 = final image size. 120 = desired layer size\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(120, num_classes)\n",
    "        \n",
    "\n",
    "     # define order of the layers\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.max_pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.max_pool2(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        # softmax already impleneted into cross entropy loss function \n",
    "     \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Train + Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: want to validate the model on data it hasn't been trained on => split data into train and valid dataset.\n",
    "# Terms: Epoch = one run through entire training dataset, step = one batch of data\n",
    "\n",
    "# general idea: Load data in model in batches, then calculate loss and perform backpropagation to modify weights starting from last layer to minimize that loss\n",
    "\n",
    "# training parameters\n",
    "num_epochs = 10\n",
    "train_losses, val_losses = [], []\n",
    "model = CardClassifierCNN()\n",
    "print(model)\n",
    "model.to(device) # utilize processor\n",
    "criterion = nn.CrossEntropyLoss() # loss function (what model optimizes to minimize loss)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader, desc='Training loop'):\n",
    "            images, labels = images.to(device), labels.to(device)    \n",
    "            \n",
    "            #forward pass\n",
    "            outputs = model(images) # call forward on the images\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # backpropagation to update model weight\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc='Validation loop'):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "        val_loss = running_loss / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss}, Validation loss: {val_loss}\")\n",
    "else: # load in already trained model\n",
    "    # model = timm.create_model('efficientnet_b0', pretrained=False)  # define architetcture of model\n",
    "    model_weights_path = \"model.pth\"\n",
    "    model.load_state_dict(torch.load(model_weights_path))\n",
    "    model.eval() # set to eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Loss\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss over epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)), # resize input image to desired pixel dimensions\n",
    "    transforms.ToTensor(), # converts pixel RGB val from [0,255] -> [0,1]\n",
    "])\n",
    "\n",
    "def preprocess_image(image_path, transform):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return image, transform(image).unsqueeze(0)\n",
    "\n",
    "# Predict using the model\n",
    "def predict(model, image_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        probabilities = probabilities.cpu().numpy().flatten()\n",
    "        max_idx = np.argmax(probabilities)\n",
    "        predicted_class = train_dataset.classes[max_idx]\n",
    "    return predicted_class, probabilities\n",
    "\n",
    "# Visualization\n",
    "def visualize_predictions(original_image, predicted_class, probabilities, class_names):\n",
    "    fig, axarr = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    \n",
    "    # Display image\n",
    "    axarr[0].imshow(original_image)\n",
    "    axarr[0].axis(\"off\")\n",
    "    axarr[0].set_title(f\"Prediction: {predicted_class}\")\n",
    "    \n",
    "    # Display predictions\n",
    "    axarr[1].barh(class_names, probabilities)\n",
    "    axarr[1].set_xlabel(\"Probability\")\n",
    "    axarr[1].set_title(\"Class Predictions\")\n",
    "    axarr[1].set_xlim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy using test data\n",
    "label_to_class = {v: k for k, v in ImageFolder(test_dir).class_to_idx.items()}\n",
    "\n",
    "sample_size = 100\n",
    "num_test_images = 0\n",
    "num_correct = 0\n",
    "random_idxes = random.sample(range(0, len(test_dataset)), sample_size)\n",
    "\n",
    "for i in range(sample_size):\n",
    "    if num_test_images == sample_size:\n",
    "        break\n",
    "    random_idx = random_idxes[i]\n",
    "    image, label = test_dataset[random_idx]\n",
    "    image_tensor = image.unsqueeze(0) # makes it a batch size of 1 when model expects a batch of inages as input\n",
    "    predicted_class, probabilities = predict(model, image_tensor)\n",
    "    num_test_images += 1\n",
    "    if predicted_class == label_to_class[label]:\n",
    "        num_correct += 1\n",
    "    else: # display if incorrect\n",
    "        original_image = transforms.ToPILImage()(image)\n",
    "        # visualize_predictions(original_image, predicted_class, probabilities, class_names)\n",
    "\n",
    "    class_names = train_dataset.classes\n",
    "print(f\"Accuracy: {num_correct} / {sample_size} Correct ({100 * num_correct / sample_size}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "test_images = glob(\"dataset/test/*/*\")\n",
    "test_examples = np.random.choice(test_images, 1)\n",
    "\n",
    "for example in test_examples:\n",
    "    original_image, image_tensor = preprocess_image(example, transform)\n",
    "    predicted_class, probabilities = predict(model, image_tensor)\n",
    "\n",
    "    class_names = train_dataset.classes \n",
    "    visualize_predictions(original_image, predicted_class, probabilities, class_names)\n",
    "\n",
    "# manual test\n",
    "\n",
    "def preprocess_user_image(image_path, transform):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    # image = image.rotate(90) # must be sideways\n",
    "    return image, transform(image).unsqueeze(0)\n",
    "\n",
    "# user_image_name = \"DA-2.jpg\"\n",
    "# user_image_path = f\"dataset/usertest/{user_image_name}\"\n",
    "# original_image, image_tensor = preprocess_image(user_image_path, transform)\n",
    "# print(image_tensor.shape)\n",
    "# predicted_class, probabilities = predict(model, image_tensor)\n",
    "\n",
    "# # Assuming dataset.classes gives the class names\n",
    "# class_names = train_dataset.classes \n",
    "# visualize_predictions(original_image, predicted_class, probabilities, class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Save the state_dict of the model as a pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save .pth (trained weighted sums)\n",
    "if train:\n",
    "    model_path = \"model.pth\"\n",
    "    model.eval()\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: To use on STM32, convert to tflite model and quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nvidia-pyindex \n",
    "!pip3 install onnx_graphsurgeon onnx2tf sng4onnx onnxsim tensorflow==2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import onnx2tf\n",
    "\n",
    "input_shape = (1, 3, 128, 128)\n",
    "\n",
    "# Step 1: Convert PyTorch model to ONNX\n",
    "dummy_input = torch.randn(input_shape)  # Example input tensor\n",
    "model_cpu = model\n",
    "model_cpu.to('cpu')\n",
    "torch.onnx.export(model_cpu, dummy_input, \"model.onnx\", export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[07mModel optimizing started\u001b[0m ============================================================\n",
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Constant   │ 8              │ 8                │\n",
      "│ Conv       │ 2              │ 2                │\n",
      "│ Flatten    │ 1              │ 1                │\n",
      "│ Gemm       │ 2              │ 2                │\n",
      "│ MaxPool    │ 2              │ 2                │\n",
      "│ Relu       │ 3              │ 3                │\n",
      "│ Model Size │ 1.7MiB         │ 1.7MiB           │\n",
      "└────────────┴────────────────┴─────��────────────┘\n",
      "\n",
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Constant   │ 8              │ 8                │\n",
      "│ Conv       │ 2              │ 2                │\n",
      "│ Flatten    │ 1              │ 1                │\n",
      "│ Gemm       │ 2              │ 2                │\n",
      "│ MaxPool    │ 2              │ 2                │\n",
      "│ Relu       │ 3              │ 3                │\n",
      "│ Model Size │ 1.7MiB         │ 1.7MiB           │\n",
      "└────────────┴────────────────┴──────────────────┘\n",
      "\n",
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Constant   │ 8              │ 8                │\n",
      "│ Conv       │ 2              │ 2                │\n",
      "│ Flatten    │ 1              │ 1                │\n",
      "│ Gemm       │ 2              │ 2                │\n",
      "│ MaxPool    │ 2              │ 2                │\n",
      "│ Relu       │ 3              │ 3                │\n",
      "│ Model Size │ 1.7MiB         │ 1.7MiB           │\n",
      "└────────────┴────────────────┴──────────────────┘\n",
      "\n",
      "\u001b[32mModel optimizing complete!\u001b[0m\n",
      "\n",
      "\u001b[07mAutomatic generation of each OP name started\u001b[0m ========================================\n",
      "\u001b[32mAutomatic generation of each OP name complete!\u001b[0m\n",
      "\n",
      "\u001b[07mModel loaded\u001b[0m ========================================================================\n",
      "\n",
      "\u001b[07mModel conversion started\u001b[0m ============================================================\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32minput_op_name\u001b[0m: input.1 \u001b[32mshape\u001b[0m: [1, 3, 128, 128] \u001b[32mdtype\u001b[0m: float32\n",
      "\u001b[33mWARNING:\u001b[0m The optimization process for shape estimation is skipped because it contains OPs that cannot be inferred by the standard onnxruntime.\n",
      "\u001b[33mWARNING:\u001b[0m [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /Users/runner/work/1/s/onnxruntime/core/graph/model.cc:180 onnxruntime::Model::Model(ModelProto &&, const PathString &, const IOnnxRuntimeOpSchemaRegistryList *, const logging::Logger &, const ModelOptions &) Unsupported model IR version: 10, max supported IR version: 9\n",
      "\n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m2 / 11\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Conv\u001b[35m onnx_op_name\u001b[0m: wa/conv1/Conv\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: input.1 \u001b[36mshape\u001b[0m: [1, 3, 128, 128] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: conv1.weight \u001b[36mshape\u001b[0m: [6, 3, 5, 5] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: conv1.bias \u001b[36mshape\u001b[0m: [6] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/conv1/Conv_output_0 \u001b[36mshape\u001b[0m: [1, 6, 62, 62] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: convolution_v2\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input\u001b[0m: \u001b[34mname\u001b[0m: input.1 \u001b[34mshape\u001b[0m: (1, 128, 128, 3) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.weights\u001b[0m: \u001b[34mshape\u001b[0m: (5, 5, 3, 6) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.bias\u001b[0m: \u001b[34mshape\u001b[0m: (6,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.strides\u001b[0m: \u001b[34mval\u001b[0m: [2, 2] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.dilations\u001b[0m: \u001b[34mval\u001b[0m: [1, 1] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.6.padding\u001b[0m: \u001b[34mval\u001b[0m: VALID \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.7.group\u001b[0m: \u001b[34mval\u001b[0m: 1 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add/Add:0 \u001b[34mshape\u001b[0m: (1, 62, 62, 6) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m3 / 11\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Relu\u001b[35m onnx_op_name\u001b[0m: wa/relu1/Relu\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/conv1/Conv_output_0 \u001b[36mshape\u001b[0m: [1, 6, 62, 62] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/relu1/Relu_output_0 \u001b[36mshape\u001b[0m: [1, 6, 62, 62] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: relu\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.features\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add/Add:0 \u001b[34mshape\u001b[0m: (1, 62, 62, 6) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.relu/Relu:0 \u001b[34mshape\u001b[0m: (1, 62, 62, 6) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m4 / 11\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: MaxPool\u001b[35m onnx_op_name\u001b[0m: wa/max_pool1/MaxPool\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/relu1/Relu_output_0 \u001b[36mshape\u001b[0m: [1, 6, 62, 62] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/max_pool1/MaxPool_output_0 \u001b[36mshape\u001b[0m: [1, 6, 31, 31] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: max_pool_v2\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.relu/Relu:0 \u001b[34mshape\u001b[0m: (1, 62, 62, 6) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.filters\u001b[0m: \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.kernel_shape\u001b[0m: \u001b[34mval\u001b[0m: [2, 2] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.strides\u001b[0m: \u001b[34mval\u001b[0m: [2, 2] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.dilations\u001b[0m: \u001b[34mval\u001b[0m: [1, 1] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.6.padding\u001b[0m: \u001b[34mval\u001b[0m: [0, 0, 0, 0] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.7.ceil_mode\u001b[0m: \u001b[34mval\u001b[0m: False \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output0\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.max_pool2d/MaxPool2d:0 \u001b[34mshape\u001b[0m: (1, 31, 31, 6) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.2.output1\u001b[0m: \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m5 / 11\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Conv\u001b[35m onnx_op_name\u001b[0m: wa/conv2/Conv\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/max_pool1/MaxPool_output_0 \u001b[36mshape\u001b[0m: [1, 6, 31, 31] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: conv2.weight \u001b[36mshape\u001b[0m: [16, 6, 2, 2] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: conv2.bias \u001b[36mshape\u001b[0m: [16] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/conv2/Conv_output_0 \u001b[36mshape\u001b[0m: [1, 16, 30, 30] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: convolution_v2\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.max_pool2d/MaxPool2d:0 \u001b[34mshape\u001b[0m: (1, 31, 31, 6) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.weights\u001b[0m: \u001b[34mshape\u001b[0m: (2, 2, 6, 16) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.bias\u001b[0m: \u001b[34mshape\u001b[0m: (16,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.strides\u001b[0m: \u001b[34mval\u001b[0m: [1, 1] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.dilations\u001b[0m: \u001b[34mval\u001b[0m: [1, 1] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.6.padding\u001b[0m: \u001b[34mval\u001b[0m: VALID \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.7.group\u001b[0m: \u001b[34mval\u001b[0m: 1 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_1/Add:0 \u001b[34mshape\u001b[0m: (1, 30, 30, 16) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m6 / 11\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Relu\u001b[35m onnx_op_name\u001b[0m: wa/relu2/Relu\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/conv2/Conv_output_0 \u001b[36mshape\u001b[0m: [1, 16, 30, 30] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/relu2/Relu_output_0 \u001b[36mshape\u001b[0m: [1, 16, 30, 30] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: relu\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.features\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_1/Add:0 \u001b[34mshape\u001b[0m: (1, 30, 30, 16) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.relu_1/Relu:0 \u001b[34mshape\u001b[0m: (1, 30, 30, 16) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m7 / 11\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: MaxPool\u001b[35m onnx_op_name\u001b[0m: wa/max_pool2/MaxPool\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/relu2/Relu_output_0 \u001b[36mshape\u001b[0m: [1, 16, 30, 30] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/max_pool2/MaxPool_output_0 \u001b[36mshape\u001b[0m: [1, 16, 15, 15] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: max_pool_v2\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.relu_1/Relu:0 \u001b[34mshape\u001b[0m: (1, 30, 30, 16) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.filters\u001b[0m: \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.kernel_shape\u001b[0m: \u001b[34mval\u001b[0m: [2, 2] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.strides\u001b[0m: \u001b[34mval\u001b[0m: [2, 2] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.dilations\u001b[0m: \u001b[34mval\u001b[0m: [1, 1] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.6.padding\u001b[0m: \u001b[34mval\u001b[0m: [0, 0, 0, 0] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.7.ceil_mode\u001b[0m: \u001b[34mval\u001b[0m: False \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output0\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.max_pool2d_1/MaxPool2d:0 \u001b[34mshape\u001b[0m: (1, 15, 15, 16) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.2.output1\u001b[0m: \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m8 / 11\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Flatten\u001b[35m onnx_op_name\u001b[0m: wa/flatten/Flatten\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/max_pool2/MaxPool_output_0 \u001b[36mshape\u001b[0m: [1, 16, 15, 15] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 3600] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: reshape\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.transpose/transpose:0 \u001b[34mshape\u001b[0m: (1, 16, 15, 15) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.shape\u001b[0m: \u001b[34mval\u001b[0m: [1, 3600] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape/Reshape:0 \u001b[34mshape\u001b[0m: (1, 3600) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m9 / 11\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/fc3/Gemm\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/flatten/Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 3600] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: fc3.weight \u001b[36mshape\u001b[0m: [120, 3600] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: fc3.bias \u001b[36mshape\u001b[0m: [120] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/fc3/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 120] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 3600) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (3600, 120) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (120,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add/AddV2:0 \u001b[34mshape\u001b[0m: (1, 120) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m10 / 11\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Relu\u001b[35m onnx_op_name\u001b[0m: wa/relu3/Relu\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/fc3/Gemm_output_0 \u001b[36mshape\u001b[0m: [1, 120] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/relu3/Relu_output_0 \u001b[36mshape\u001b[0m: [1, 120] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: relu\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.features\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add/AddV2:0 \u001b[34mshape\u001b[0m: (1, 120) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.relu_2/Relu:0 \u001b[34mshape\u001b[0m: (1, 120) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m11 / 11\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: wa/fc4/Gemm\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/relu3/Relu_output_0 \u001b[36mshape\u001b[0m: [1, 120] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: fc4.weight \u001b[36mshape\u001b[0m: [54, 120] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: fc4.bias \u001b[36mshape\u001b[0m: [54] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: 18 \u001b[36mshape\u001b[0m: [1, 54] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 120) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (120, 54) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (54,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_1/AddV2:0 \u001b[34mshape\u001b[0m: (1, 54) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[07msaved_model output started\u001b[0m ==========================================================\n",
      "\u001b[32msaved_model output complete!\u001b[0m\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 10, Total Ops 21, % non-converted = 47.62 %\n",
      " * 10 ARITH ops\n",
      "\n",
      "- arith.constant:   10 occurrences  (f32: 8, i32: 2)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "\u001b[32mFloat32 tflite output complete!\u001b[0m\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 10, Total Ops 29, % non-converted = 34.48 %\n",
      " * 10 ARITH ops\n",
      "\n",
      "- arith.constant:   10 occurrences  (f16: 8, i32: 2)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 8)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "\u001b[32mFloat16 tflite output complete!\u001b[0m\n",
      "\u001b[31mERROR:\u001b[0m If --check_onnx_tf_outputs_elementwise_close is specified, you must install onnxruntime and sne4onnx. pip install sne4onnx onnxruntime\n"
     ]
    }
   ],
   "source": [
    "# Step 2:Convert ONNX -> TF\n",
    "!onnx2tf -i model.onnx -b 1 -osd -cotof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tf.Tensor(\n",
      "[[ -19.595432    -80.46757     -31.8682      -37.43191       7.387253\n",
      "    10.831307     17.182154     26.335203     -2.496031    -57.03158\n",
      "   -33.13178       9.184201      0.33130112    9.106157   -112.20677\n",
      "   -96.52209     -80.673744    -46.608517    -41.014503    -11.544881\n",
      "    13.6490965   -31.093334   -110.535866    -27.811996    -10.164543\n",
      "    -4.2750688     1.3099015  -103.08112     -75.242905    -86.937355\n",
      "   -55.802193    -29.768412     -4.029815      7.014435    -15.637187\n",
      "   -93.032684    -26.811283     -4.234797    -22.255192    -14.785543\n",
      "   -10.635503    -60.972683    -37.15684     -40.444305    -20.48801\n",
      "    -5.9094553     9.44023      27.113758     -0.38616794    3.2199535\n",
      "   -30.491875    -29.352875    -28.318184     17.63939   ]], shape=(1, 54), dtype=float32)\n",
      "prediction: S8 \n"
     ]
    }
   ],
   "source": [
    "# test tf model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((128, 128))  # Resize the image to match model input size\n",
    "    img = (np.array(img) / 255.0).astype(np.float32)  # Normalize pixel values to [0, 1], and is type float32\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension # tf tensor = (1, 128, 128, 3)\n",
    "    return img\n",
    "\n",
    "# Test the model with an input image\n",
    "def test_model_with_image(image_path, model):\n",
    "    # Preprocess the image\n",
    "    img = preprocess_image(image_path)\n",
    "    # Perform inference\n",
    "    prediction = model(img)\n",
    "    return prediction\n",
    "\n",
    "# Load tf model\n",
    "model = tf.saved_model.load(\"saved_model\")\n",
    "\n",
    "image_name = \"C8/C8-2.jpg\"\n",
    "image_path = f\"dataset/test/{image_name}\"\n",
    "output_data = test_model_with_image(image_path, model)\n",
    "print(\"Output:\", output_data)\n",
    "\n",
    "max_index = np.argmax(output_data)\n",
    "label_to_class = {v: k for k, v in ImageFolder(test_dir).class_to_idx.items()}\n",
    "\n",
    "print(f\"prediction: {label_to_class[max_index]} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 10, Total Ops 21, % non-converted = 47.62 %\n",
      " * 10 ARITH ops\n",
      "\n",
      "- arith.constant:   10 occurrences  (f32: 8, i32: 2)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "# convert tf -> tflite\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "def rep_dataset():\n",
    "    \"\"\"Generator function to produce representative dataset for post-training quantization.\"\"\"\n",
    "    subset_indices = list(range(100))  # Indices of the first 100 samples\n",
    "    subset = Subset(valid_dataset, subset_indices)\n",
    "    # Use a few samples from the training set.\n",
    "    for image_tensor, label in subset: # pytorch image tensor = (1,3, 128,128)\n",
    "        image_tensor = image_tensor.unsqueeze(0) # add batch dimension\n",
    "        image_tensor = image_tensor.permute(0, 2, 3, 1)  # Transpose dimensions to match tensor flow order of (1, 128, 128, 3)\n",
    "        yield [tf.dtypes.cast(image_tensor, tf.float32)]\n",
    "\n",
    "\n",
    "# Quantize the TF model = 8-bit linear quantization of an NN model \n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"saved_model\")\n",
    "converter.signature_key = \"serving_default\"\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] # st might've warned not to use this. try commneting out if dones twork\n",
    "converter.representative_dataset = rep_dataset\n",
    "\n",
    "# Ensure that if ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "converter.inference_input_type = tf.float32 # note: uint8 is far too inaccuarte\n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "# Convert TF -> TFLITE\n",
    "tflite_quantized_model = converter.convert()\n",
    "with open(\"model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [0.4468085  0.06382979 0.46808514 0.36170214 0.71276593 0.53191495\n",
      " 0.5957447  0.40425533 0.5957447  0.80851066 0.32978725 0.28723404\n",
      " 0.20212767 0.29787236 0.12765959 0.26595747 0.37234044 0.6702128\n",
      " 0.6489362  0.51063836 0.7659575  1.         0.6489362  0.21276596\n",
      " 0.6489362  0.12765959 0.82978725 0.         0.29787236 0.39361703\n",
      " 0.5638298  0.6914894  0.62765956 0.30851066 0.85106385 0.606383\n",
      " 0.09574466 0.51063836 0.29787236 0.5638298  0.52127665 0.26595747\n",
      " 0.5531915  0.51063836 0.7340426  0.5531915  0.58510643 0.39361703\n",
      " 0.81914896 0.5        0.12765959 0.30851066 0.31914893 0.6489362 ]\n",
      "D9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "# test tflite model\n",
    "\n",
    "def preprocess_image(image_path, input_details):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((input_details[0]['shape'][2], input_details[0]['shape'][1]))  # Resize image to model input shape (128, 128)\n",
    "    img = (np.array(img) / 255.0).astype(np.float32)  # Normalize pixel values to [0, 1], and is type float32 # note: uint8 is far too inaccuarte\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "def tflite_predict(image_tensor, interpreter):    \n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], image_tensor) # Set input tensor\n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "    # Get the output tensor\n",
    "    probabilities = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "\n",
    "    min_val = probabilities.min()\n",
    "    max_val = probabilities.max()\n",
    "    normalized_probabilities = (probabilities - min_val) / (max_val - min_val)\n",
    "    max_idx = np.argmax(normalized_probabilities)\n",
    "    predicted_class = train_dataset.classes[max_idx]\n",
    "    \n",
    "    return predicted_class, normalized_probabilities\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "\n",
    "image_name = \"D9/D9-7.jpg\"\n",
    "image_path = f\"dataset/test/{image_name}\"\n",
    "image_tensor = preprocess_image(image_path, input_details)\n",
    "predicted_class, probabilities = tflite_predict(image_tensor, interpreter)\n",
    "\n",
    "print(\"Output:\", probabilities)\n",
    "print(predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80 / 100 Correct (80.0%)\n"
     ]
    }
   ],
   "source": [
    "# assess tflite model \n",
    "\n",
    "label_to_class = {v: k for k, v in ImageFolder(test_dir).class_to_idx.items()}\n",
    "\n",
    "sample_size = 1000\n",
    "num_test_images = 0\n",
    "num_correct = 0\n",
    "random_idxes = random.sample(range(0, len(test_dataset)), sample_size)\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "for i in range(sample_size):\n",
    "    if num_test_images == sample_size:\n",
    "        break\n",
    "    random_idx = random_idxes[i]\n",
    "    image, label = test_dataset[random_idx]\n",
    "    image_tensor = image.unsqueeze(0) # makes it a batch size of 1 when model expects a batch of inages as input\n",
    "    image_tensor = image_tensor.permute(0, 2, 3, 1)  # Transpose dimensions to match tensor flow order of (1, 128, 128, 3)\n",
    "    predicted_class, probabilities = tflite_predict(image_tensor, interpreter)\n",
    "    num_test_images += 1\n",
    "    if predicted_class == label_to_class[label]:\n",
    "        num_correct += 1\n",
    "    else: # display if incorrect\n",
    "        original_image = transforms.ToPILImage()(image)\n",
    "        # visualize_predictions(original_image, predicted_class, probabilities, class_names)\n",
    "\n",
    "print(f\"Accuracy: {num_correct} / {sample_size} Correct ({100 * num_correct / sample_size}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
